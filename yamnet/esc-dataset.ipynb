{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9141920,"sourceType":"datasetVersion","datasetId":5521458}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\nimport torchaudio\nimport pandas as pd\nimport os\nimport torchvision","metadata":{"_uuid":"232dab9d-9d7c-4565-a929-e23bfbb74890","_cell_guid":"e9653ff6-9e9f-41f0-9b81-24a2a911fde0","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-11T18:44:04.461726Z","iopub.execute_input":"2024-08-11T18:44:04.462516Z","iopub.status.idle":"2024-08-11T18:44:11.841223Z","shell.execute_reply.started":"2024-08-11T18:44:04.462474Z","shell.execute_reply":"2024-08-11T18:44:11.839997Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift,Shift","metadata":{"_uuid":"10a34546-5ce3-4c22-99df-f5b3db0983c0","_cell_guid":"8c901156-d4a1-453a-a090-fed9293de174","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-11T18:44:11.843094Z","iopub.execute_input":"2024-08-11T18:44:11.843884Z","iopub.status.idle":"2024-08-11T18:44:12.148584Z","shell.execute_reply.started":"2024-08-11T18:44:11.843848Z","shell.execute_reply":"2024-08-11T18:44:12.146360Z"},"trusted":true},"execution_count":2,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maudiomentations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Compose, AddGaussianNoise, TimeStretch, PitchShift,Shift\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'audiomentations'"],"ename":"ModuleNotFoundError","evalue":"No module named 'audiomentations'","output_type":"error"}]},{"cell_type":"code","source":"SAMPLE_RATE = 22050","metadata":{"_uuid":"e4a9fbe5-ae31-4c65-8773-e6cbfb66d8f4","_cell_guid":"bccf69f0-42bf-485b-9e46-1bf5479430d8","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-11T18:44:12.149651Z","iopub.status.idle":"2024-08-11T18:44:12.150172Z","shell.execute_reply.started":"2024-08-11T18:44:12.149933Z","shell.execute_reply":"2024-08-11T18:44:12.149952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class RepeatChannelsTransform:\n    def __call__(self,img):\n        return img.repeat(3, 1, 1)\nmin_max_scaling = torchvision.transforms.Lambda(lambda x: (x - x.min()) / (x.max() - x.min()))","metadata":{"execution":{"iopub.status.busy":"2024-08-11T18:44:12.151966Z","iopub.status.idle":"2024-08-11T18:44:12.152474Z","shell.execute_reply.started":"2024-08-11T18:44:12.152249Z","shell.execute_reply":"2024-08-11T18:44:12.152269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"default_augmenter = Compose([\n    AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.1,p=0.2),\n    Shift(min_shift=-1, max_shift=1, p=0.7),\n    TimeStretch(min_rate=0.8, max_rate=1.2, p=0.6),\n    PitchShift(min_semitones=-4, max_semitones=4, p=0.6)\n])","metadata":{"_uuid":"ff90cc8c-5373-4450-80f8-ac63f0bf6ead","_cell_guid":"949c3ebc-1c04-4ef5-8ff2-1638e9f26a0a","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-11T18:44:12.154885Z","iopub.status.idle":"2024-08-11T18:44:12.155447Z","shell.execute_reply.started":"2024-08-11T18:44:12.155136Z","shell.execute_reply":"2024-08-11T18:44:12.155155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ESCDataset(Dataset):\n    def __init__(self,\n                 annotations_file,\n                 audio_dir,\n                 train,\n                 target_sample_rate = SAMPLE_RATE,\n                 augmenter = default_augmenter,\n                 device = 'cpu'):\n        self.device = device\n        self.annotations = pd.read_csv(annotations_file)\n        self.train = train\n        if train:\n            self.annotations = self.annotations[self.annotations['fold'] != 5]\n        else :\n            self.annotations = self.annotations[self.annotations['fold'] == 5]\n        self.audio_dir = audio_dir\n        self.augmenter = augmenter\n        self.target_sample_rate = target_sample_rate\n        self.define_transforms()\n    \n    def define_transforms(self):\n        self.mel_spectrogram_transform = torchaudio.transforms.MelSpectrogram(\n                sample_rate = SAMPLE_RATE,\n                n_fft = 2048,\n                hop_length = 512,\n                n_mels = 128\n            ).to(self.device)\n        self.log_spectrogram_transform = torchaudio.transforms.AmplitudeToDB().to(self.device)\n        self.vision_transforms = torchvision.transforms.Compose([\n                torchvision.transforms.Resize((224, 224),antialias=True).to(self.device),\n                RepeatChannelsTransform(),\n                min_max_scaling\n            ])\n\n    def __len__(self):\n        return len(self.annotations)\n\n    def __getitem__(self,idx):\n        sample_path = self._get_sample_path(idx)\n        label = self._get_sample_label(idx)\n        signal, sr = torchaudio.load(sample_path)\n        if self.train:\n            signal = torch.from_numpy(self.augmenter(signal.numpy(),sr))\n        signal = signal.to(self.device)\n        signal = self._fix_sample_rate(signal,sr)\n        \n        signal = self.mel_spectrogram_transform(signal)\n        signal = self.vision_transforms(signal)        \n        return signal,label\n\n    def _fix_sample_rate(self,signal,sr):\n        if sr != self.target_sample_rate:\n            resampler = torchaudio.transforms.Resample(sr,self.target_sample_rate).to(self.device)\n            signal = resampler(signal)\n        return signal\n\n    def _get_sample_path(self,idx):\n        path = os.path.join(self.audio_dir,self.annotations.iloc[idx,0])\n        return path\n\n    def _get_sample_label(self,idx):\n        return self.annotations.iloc[idx,2]","metadata":{"_uuid":"1b8e0c93-ad13-4ee0-800a-fdb3542e90df","_cell_guid":"973b3eef-8cdf-40b8-add4-a42f393e2847","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-10T10:57:00.715515Z","iopub.execute_input":"2024-08-10T10:57:00.715929Z","iopub.status.idle":"2024-08-10T10:57:00.730597Z","shell.execute_reply.started":"2024-08-10T10:57:00.715897Z","shell.execute_reply":"2024-08-10T10:57:00.729331Z"},"trusted":true},"execution_count":6,"outputs":[]}]}